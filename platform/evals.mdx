---
title: Evals
description: Evaluate and benchmark SmartAgents to prevent regressions
icon: "flask"
---

**Evals** are used to evaluate and benchmark SmartAgents to ensure that updates and improvements do not cause regressions. It enables systematic testing of SmartAgent versions to track performance over time.

## Overview

Evals help teams maintain and improve SmartAgent quality through continuous evaluation. By running standardized tests against different versions, you can ensure that changes enhance rather than degrade performance.

<img src="/platform/images/testing-suite-interface.png" alt="Eval Interface" />

## Key Concepts

### Eval

A collection of messages and their corresponding inputs, used to test SmartAgents.

**Characteristics:**

- Created from real-world conversation data
- Can be tailored for specific purposes (e.g., all greetings, all resolutions)
- May represent a "golden standard" sampling of many conversation types
- Reusable across multiple test runs

**Example Evals:**

- "HelloFresh Greetings" - All greeting interactions
- "HelloFresh Resolution" - Complete resolution scenarios
- "HelloFresh Agent Response Evaluation" - Comprehensive multi-scenario testing

### Test

An individual message and its input, drawn from the eval dataset.

**Components:**

- Input message from customer
- Expected context or background data
- Evaluation criteria
- Pass/fail conditions

### Run

The execution of an eval's tests against a specific version of a SmartAgent.

**Outputs:**

- Pass/fail rate
- Performance metrics
- Individual test results
- Comparison to previous runs

## Purpose

Evals enable continuous improvement by tracking incremental changes to ensure SmartAgents get better over time, not worse. It helps identify unintended performance drops after changes, benchmark new versions against historical averages, and ensure improvements in one area don't negatively impact unrelated functionality.

## Use Cases

### Before Deployment

- Validate changes before pushing to production
- Compare new version against current production baseline
- Ensure pass rate meets minimum thresholds

### Continuous Monitoring

- Run regular evals to catch unexpected regressions
- Track performance trends over time
- Build confidence in SmartAgent reliability

### Targeted Improvements

- Test specific conversation types (greetings, resolutions, etc.)
- Validate fixes for known issues
- Ensure edge cases are handled correctly

### Customer Reporting

- Demonstrate quality improvements to stakeholders
- Show performance metrics over time
- Provide evidence of continuous optimization

## Workflow

1. **Create Eval** - Build a collection of test messages from real conversations
2. **Define Tests** - Specify individual test cases with expected outcomes
3. **Run Against Version** - Execute the eval against a SmartAgent version
4. **Review Results** - Analyze pass/fail rates and individual test outcomes
5. **Compare Versions** - See how new versions perform vs. previous versions
6. **Deploy with Confidence** - Push changes knowing they've been validated

## Best Practices

1. **Use Real Data** - Create evals from actual customer conversations
2. **Test Regularly** - Run evals before every deployment
3. **Track Over Time** - Monitor trends to ensure continuous improvement
4. **Diversify Test Cases** - Include edge cases and common scenarios
5. **Set Clear Criteria** - Define what "passing" means for each test
6. **Document Baselines** - Know your starting point for each eval

## Benefits

By continuously running evals supports:

- **Consistent upward progress** in SmartAgent quality
- **Prevention of regressions** where fixing one area breaks another
- **Data-driven optimization** based on real performance metrics
- **Confidence in deployments** through systematic validation

## Related Pages

- [SmartAgent Builder](/platform/smartAgentBuilder) - Build and deploy SmartAgent versions
- [Quality](/platform/qaQueue) - Review and provide feedback on conversations
- [Settings](/platform/settings) - Configure testing environments
